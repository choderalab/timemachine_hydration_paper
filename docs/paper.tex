\documentclass{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{amsmath}
\usepackage{physics}
\usepackage{bm}

\usepackage{graphicx}
\begin{document}

\centerline{\sc \large Analytic derivatives of the trajectory with respect to forcefield parameters.}
\vspace{.5pc}
\centerline{An exact solution for fitting parameters to ensemble observables.}
\vspace{.5pc}
\centerline{Yutong Zhao (proteneer@gmail.com), unaffiliated}
\vspace{2pc}

\section{Introduction}

When we run molecular dynamics to generate conformations along a trajectory, we almost always generate observables derived from the ensemble. Example observables span from simple quantities such as pseudo-electron density in XRD/ED refinement, density, radii of gyration, Helmholtz free energies, distance information arising from NMR, etc. Nearly always, the experimentally measured observable and the predicted observable don't agree with each other. This results in the painstaking effort of identifying new forcefield functional forms and/or fitting their parameters. Note that this paper is not concerned with the former (discovery of functional forms), but rather, on how to fit the parameters used by these functional forms to condensed phase properties that arise from a simulation (which often span from simple fixed-charged models, all the way to more complicated polarizable forcefields).

In stark contrast to training techniques of modern machine learning, forcefield parameter fitting in MD has been sort of a black-art. At a high level, the parameter tuning procedure is typically separated into two parts: 1) fitting to gas phase QM energies across a varying level of theories either by energy or force-matching, which initializes parameters into a sane region of parameter space, 2) followed by a difficult procedure of fitting to condensed phase properties in a variety of solvent and/or crystallographic conditions. Typically, the second step requires repeated simulations to generate predicted observables from the ensemble. However, there lacks an analytic derivative of the observables to parameters partially owing to complexity of generating necessary derivatives.

Fortunately, with the advent of sophisticated auto differentiation systems in modern machine learning frameworks, it has become not only possible, but also practical to generate these derivatives. We introduce a new molecular dynamics tool with two primary aims:

First, the ability to quickly test new functional forms, as well as its derivatives. Ideally, the user should only need to specify the functional form of the energy function, and all subsequent higher order derivatives should be automatically generated. As an example, OpenMM uses symbolic differentiation (lepton) that's then JIT compiled into the respective kernels. However, symbolic differentiation has severe drawbacks in that it scales poorly with increased complexity of functional forms. In contrast, forward/reverse-mode autograd systems uses dual-algebra, which tends to scale better with complexity as well as naturally lending itself to higher order derivatives. This should enable us to rapidly prototype novel functional forms, ranging from polarizable models to neural-network potentials.

Second, the ability to analytically fit these parameters to observables derived from a simulation trajectory. We show that one can, without too much difficulty, implement analytic derivatives through Langevin dynamics provided one has access to analytic hessians and second-order mixed partial derivatives of the energy function. This allows for optimization of not only the location of the minimas, but also the curvature along the path to reach said minimias. This approach is in contrast to other approaches such as thermodynamic gradients (assuming the ergodic limit has been reached), which uses only the first order derivative of the energy with respect to the parameters. In practice, the implementation is realized in two parts through the chain rule, the first being the derivative of the observable with respect to each conformation in the ensemble, the second being the derivative of a conformation in the trajectory with respect to the forcefield parameters.

\section{Mathematical Formalism}

Define the loss function $L(O(\vec{X}_{label}), O(\vec{X}_{md}))$, where $O$ is a function that calculates some observable from an ensemble. $\vec{X}_{md}(\vec{x}_0; \theta)$ is a set of conformations derived from a molecular dynamics simulation, starting from some initial state $\vec{x}_0$ with some set of forcefield parameters $\theta$. We can also write $\vec{X}_{md}$ as a collection of individual states $\vec{x}_t$ at various times $t$. The goal is to analytically derive $\dfrac{\partial L}{\partial \theta}$ so we can train $\theta$ to some experimental observable $O(\vec{X}_{label})$ by using a standard MSE loss:

\begin{equation}
L(O(\vec{x}_{label}), O(\vec{x}_t)) = \lbrack O(\vec{x}_{label}) - O(\vec{X}(\vec{x}_0;\theta)) \rbrack ^2
\end{equation}
Training the parameters means we need the gradient of $L$ with respect to the parameters $\theta$, which we compute using the chain rule:
\begin{equation}
\dfrac {\partial L}{\partial \theta} = \dfrac{\partial L}{\partial \vec{X}_{md}} \dfrac{\partial \vec{X}_{md}}{\partial \theta}
\end{equation}
In an unbiased sample of conformations, this is equal to a sum of the derivatives of individual states from the trajectory:
\begin{equation}
\dfrac {\partial L}{\partial \theta} = \sum_{t}{\dfrac{\partial L}{\partial \vec{x}_t} \dfrac{\partial \vec{x}_t}{\partial \theta}}
\end{equation}
Each summand is a component-wise matrix-multiply, where both $\partial L / \partial \vec{x}_t$ and ${\partial \vec{x}_t}/{\partial \theta}$ are $(N, 3)$ matrices. The LHS is the derivative of the loss w.r.t. to a conformation, and the RHS is the derivative of a conformation w.r.t. to the model parameters. The LHS is trivial to compute, so the RHS is the main quantity of interest.

If we were to run Langevin dynamics, the velocity is $\vec{v}_t$ is updated according to:
\begin{equation}
\vec{v}_t = a \vec{v}_{t-1} - b \nabla E(\vec{x}_{t-1};\theta) + c \vec{n}_{t-1}
\end{equation}
Where $\nabla$ is the gradient operator (w.r.t. coordinates), and $a$, $b$, $c$ are the coefficents derived from the temperature and the friction, and $\vec{n}$ is noise sampled from an independent gaussian.
\begin{equation}
\vec{x}_{t} = \vec{x}_{t-1} + \vec{v}_{t} \delta t
\end{equation}
Expanding out the first few terms we get:
\begin{equation}
\begin{split}
\vec{x}_1  &= \vec{x}_0 + \big(a \vec{v}_0 - b \nabla E(\vec{x}_0) + c \vec{n}_0  \big) \delta t \\
\vec{x}_2  &= \vec{x}_1 + \bigg( a \Big( a \vec{v}_0 - b \nabla E(\vec{x}_0) + c \vec{n}_0 \Big) - b \nabla E(\vec{x}_1) + c \vec{n}_1 \bigg) \delta t \\
\vec{x}_3  &= \vec{x}_2 + \Bigg(a\bigg(a \Big( a \vec{v}_0 - b \nabla E(\vec{x}_0) + c \vec{n}_0 \Big) - b \nabla E(\vec{x}_1) + c \vec{n}_1 \bigg) - b \nabla E(\vec{x}_2) + c \vec{n}_2 \Bigg) \delta t \\
\end{split}
\end{equation}
For convenience, we define the $\zeta$ operator:
\begin{equation}
\zeta E(\vec{x}_i; \theta) \equiv \dfrac{\partial^2E}{\partial\vec{x}_i^2}\dfrac{\partial\vec{x}_i}{\partial\theta} +  \dfrac{\partial^2 E}{\partial \vec{x}_i \partial \theta} 
\end{equation}
To demystify this expression, the first summand is a tensor contraction of a rank-4 hessian (N,3,N,3) with a rank-2 derivative (N,3). The second summand is a rank-2 mixed partial derivative (N,3). We can write out the first few terms of the derivative using the $\zeta$ operator:
\begin{equation}
\begin{split}
\dfrac{\partial\vec{x}_1}{\partial\theta}  &= - b\delta t \zeta E(\vec{x}_0) \\
\dfrac{\partial\vec{x}_2}{\partial\theta}  &= - b\delta t \Big(\zeta E(\vec{x}_0)(1+a)+\zeta E(\vec{x}_1)\Big)\\
\dfrac{\partial\vec{x}_3}{\partial\theta}  &= - b\delta t \Big(\zeta E(\vec{x}_0)(1+a+a^2)+\zeta E(\vec{x}_1)(1+a)+\zeta E(\vec{x}_2)\Big)\\
\end{split}
\end{equation}
Finally, we can tidy this up into the following expression:
\begin{equation}
\dfrac {\partial x_t}{\partial \theta} = -b  \delta t \Big( \zeta E(\vec{x}_0; \theta)S_{t-1}^a + \zeta E(\vec{x}_1;\theta)S_{t-2}^a + \ldots +  \zeta E(\vec{x}_{t-1};\theta) \Big)
\end{equation}
$S_n^a$ is the geometric series of $n$ terms of $a$. Observe that we only need to do a single forward pass, which can be implemented using forward-mode auto-differentiation. Naively, this procedure needs $O(t)$ memory and $O(t^2)$ runtime since we need to store $\partial\vec{x}_i / \partial\theta$ and traverse back in time as they're re-weighted after each step. But this would catastrophic for all practical purposes. Fortunately, we can immediately leverage the geometric series, whose rapid convergence implies we need to store the $\zeta$ terms for only a small numbers of the most recent steps, since for large $t$:
\begin{equation}
\zeta E(\vec{x}_0)(1+a+a^2+...+a^t) \approx \zeta E(\vec{x}_0)(1+a+a^2+...+a^{t-1})
\end{equation}

Suppose it takes $k$ steps for the geometric series converge to numerical accuracy. This allows us to maintain a rolling window of only size $O(k)$ for unconverged $\zeta$ values. The converged $\zeta$ terms can be reduced and thus require only $O(1)$ extra space. In practice, $k \approx 4000$, so the over all algorithm becomes becomes $O(1)$ in memory and $O(t)$ in time. 

% With this machinery in place, we can now use some additional tricks to regularize the fitting process, such as enforcing that for all ergodic systems, the time averaged observable must be equal to the ensemble averaged observable. The exact method is that we use reservoir sampling to draw from the time average, and the last time slice of a replica simulation to generate the ensemble average.

\section{Issues and Caveats}

The first major issue has to do how training proceeds. The above procedure needs to be repeated multiple times for the parameters to converge. That is, each iteration would require us to re-run an entire MD simulation since we can't easily re-use the intermediate values. For some test systems it can take anywhere from 50 to 500 rounds of simulations for a single system to be fully optimized (internal data).

A second more practical issue deals with the computational complexity and speed. The analytically dense Hessian is an inherently $O(N^2)$, which is a major increase from the standard $O(N log N)$ in MD. Furthermore, because the derivatives are automatically generated, and that it's using tensorflow underneath the hood, the simulation itself is significantly slower than a highly optimized MD package. However, with recent advances in XLA-JIT and the flexibility of implementing custom ops, we hope to reconcile this to within an order of magnitude.

\section{Code}

The code has been implemented and will be open-sourced once the manuscript has been accepted. Please contact the author for access.


\end{document}